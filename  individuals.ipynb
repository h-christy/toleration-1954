{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collecting Part2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import wikipedia\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>title</th>\n",
       "      <th>full_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name2</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>73214047</td>\n",
       "      <td>Eda Lou Walton</td>\n",
       "      <td>Eda Lou Walton</td>\n",
       "      <td>Eda Lou</td>\n",
       "      <td>Walton</td>\n",
       "      <td>see  also  Edna  Lou  Walton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>74516481</td>\n",
       "      <td>Joe Dallet</td>\n",
       "      <td>Joe Dallet</td>\n",
       "      <td>Joe</td>\n",
       "      <td>Dallet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>74616421</td>\n",
       "      <td>Jerry Tyler</td>\n",
       "      <td>Jerry Tyler</td>\n",
       "      <td>Jerry</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>75456258</td>\n",
       "      <td>Annette Rubinstein</td>\n",
       "      <td>Annette Rubinstein</td>\n",
       "      <td>Annette</td>\n",
       "      <td>Rubinstein</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>76402919</td>\n",
       "      <td>Milton Herndon</td>\n",
       "      <td>Milton Herndon</td>\n",
       "      <td>Milton</td>\n",
       "      <td>Herndon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>77527111</td>\n",
       "      <td>Guy Emery Shipler</td>\n",
       "      <td>Guy Emery Shipler</td>\n",
       "      <td>Guy Emery</td>\n",
       "      <td>Shipler</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>78698426</td>\n",
       "      <td>Joseph North (writer)</td>\n",
       "      <td>Joseph North</td>\n",
       "      <td>Joseph</td>\n",
       "      <td>North</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>78698728</td>\n",
       "      <td>Joseph Clark (journalist)</td>\n",
       "      <td>Joseph Clark</td>\n",
       "      <td>Joseph</td>\n",
       "      <td>Clark</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pageid                      title           full_name first_name  \\\n",
       "486  73214047             Eda Lou Walton      Eda Lou Walton    Eda Lou   \n",
       "487  74516481                 Joe Dallet          Joe Dallet        Joe   \n",
       "488  74616421                Jerry Tyler         Jerry Tyler      Jerry   \n",
       "489  75456258         Annette Rubinstein  Annette Rubinstein    Annette   \n",
       "490  76402919             Milton Herndon      Milton Herndon     Milton   \n",
       "491  77527111          Guy Emery Shipler   Guy Emery Shipler  Guy Emery   \n",
       "492  78698426      Joseph North (writer)        Joseph North     Joseph   \n",
       "493  78698728  Joseph Clark (journalist)        Joseph Clark     Joseph   \n",
       "\n",
       "     last_name2                      comments  \n",
       "486      Walton  see  also  Edna  Lou  Walton  \n",
       "487      Dallet                             0  \n",
       "488       Tyler                             0  \n",
       "489  Rubinstein                             0  \n",
       "490     Herndon                             0  \n",
       "491     Shipler                             0  \n",
       "492       North                             0  \n",
       "493       Clark                             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huac_df = pd.read_csv('data-tolera/494matches.csv')\n",
    "huac_df.tail(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create `wiki-info.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f1: `get_live_years()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_live_years(name):\n",
    "    try:\n",
    "        person_page = wikipedia.page(name, auto_suggest=False)\n",
    "        categories = person_page.categories\n",
    "\n",
    "        birth_year = None\n",
    "        death_year = None\n",
    "\n",
    "        birth_pattern = re.compile(r\"(\\d{4}) births\")\n",
    "        death_pattern = re.compile(r\"(\\d{4}) deaths\")\n",
    "\n",
    "        for cat in categories:\n",
    "            if not birth_year:\n",
    "                birth_match = birth_pattern.search(cat)\n",
    "                if birth_match:\n",
    "                    birth_year = int(birth_match.group(1))\n",
    "            if not death_year:\n",
    "                death_match = death_pattern.search(cat)\n",
    "                if death_match:\n",
    "                    death_year = int(death_match.group(1))\n",
    "\n",
    "            if birth_year and death_year:\n",
    "                break\n",
    "\n",
    "        return {\"name\": name, \"birth_year\": birth_year, \"death_year\": death_year}\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        return {\"error\": f\"Page for '{name}' not found.\"}\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return {\"error\": f\"Disambiguation error: {e}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Aaron_Copland', 'birth_year': 1900, 'death_year': 1990}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_live_years('Aaron_Copland')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 1: `dates_info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_info(page):\n",
    "    categories = page.categories\n",
    "\n",
    "    birth_year = None\n",
    "    death_year = None\n",
    "\n",
    "    birth_pattern = re.compile(r\"(\\d{4}) births\")\n",
    "    death_pattern = re.compile(r\"(\\d{4}) deaths\")\n",
    "\n",
    "    for cat in categories:\n",
    "        if not birth_year:\n",
    "            birth_match = birth_pattern.search(cat)\n",
    "            if birth_match:\n",
    "                birth_year = int(birth_match.group(1))\n",
    "        if not death_year:\n",
    "            death_match = death_pattern.search(cat)\n",
    "            if death_match:\n",
    "                death_year = int(death_match.group(1))\n",
    "\n",
    "        if birth_year and death_year:\n",
    "            break\n",
    "\n",
    "    return {\"birth_year\": birth_year, \"death_year\": death_year}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test with copland_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'birth_year': 1900, 'death_year': 1990}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copland_page = wikipedia.page('Aaron_Copland', auto_suggest=False)\n",
    "dates_info(copland_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 2: `sexuality_info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sexuality_info(page):\n",
    "    categories = page.categories\n",
    "    bio_sex = \"NOT_FOUND\"\n",
    "    sexuality = \"NOTA\"\n",
    "\n",
    "    male_keywords = [\"male\"]\n",
    "    female_keywords = [\"female\", 'women', 'actresses']\n",
    "    lgbtq_keywords = [\"lgbtq\", \"gay\", \"lesbian\", \"bisexual\", \"transgender\"]\n",
    "\n",
    "    br0 = 0\n",
    "    br1 = 0\n",
    "    for cat in categories:\n",
    "\n",
    "        for kw in female_keywords:\n",
    "            if kw in cat.lower():\n",
    "                bio_sex = 'FEMALE'\n",
    "                br0 = 1\n",
    "                break\n",
    "        else:\n",
    "            if br0 == 0:\n",
    "                bio_sex = 'MALE'\n",
    "\n",
    "        if any(keyword in cat.lower() for keyword in lgbtq_keywords):\n",
    "            sexuality = \"LGBTQ\"\n",
    "\n",
    "    return {\"bio_sex\": bio_sex, \"sexuality\": sexuality}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bio_sex': 'MALE', 'sexuality': 'LGBTQ'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sexuality_info(copland_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 3: `race_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_info(page):\n",
    "    categories = page.categories\n",
    "    race = \"non-black\"\n",
    "\n",
    "    black_keywords = [\"african-american\", \"black\"]\n",
    "    \n",
    "    for cat in categories:\n",
    "        category_lower = cat.lower()\n",
    "        if any(keyword in category_lower for keyword in black_keywords):\n",
    "            race = \"black\"\n",
    "            break\n",
    "\n",
    "    return {\"race\": race}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'race': 'black'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoskins_page = wikipedia.page('Allen_\"Farina\"_Hoskins', auto_suggest = False)\n",
    "race_info(hoskins_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 4: `occupation_info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occupation_info(page):\n",
    "    summary = page.summary\n",
    "\n",
    "    occupation = None\n",
    "    occupation_match = re.search(\n",
    "        r\"(was|is) (an?|the) ([A-Za-z-]+) ([A-Za-z]+(?: [A-Za-z]+)?)\",\n",
    "        summary\n",
    "        )\n",
    "    if occupation_match:\n",
    "        occupation = occupation_match.group(4).strip()\n",
    "    \n",
    "    remove_list = ['and', 'who', 'known', 'American']\n",
    "    if occupation != None:\n",
    "        for w in remove_list:\n",
    "            if w in occupation:\n",
    "                occupation = occupation.replace(w, '')\n",
    "                occupation = occupation.strip()\n",
    "\n",
    "    return {\"occupation\": occupation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'occupation': 'mathematician'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "may_page = wikipedia.page('Kenneth_O._May', auto_suggest=False)\n",
    "occupation_info(may_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extra function: `regex_match()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_match(re_patterns, lines0, group_i):\n",
    "    re_objects = []\n",
    "    for i in re_patterns:\n",
    "        object0 = re.compile(i, re.IGNORECASE)\n",
    "        re_objects.append(object0)\n",
    "    for j in range(len(re_objects)):\n",
    "        match0 = re_objects[j].search(lines0)\n",
    "        if match0:\n",
    "            match_str = match0.group(group_i[j]).strip()\n",
    "            break\n",
    "        else:\n",
    "            match_str = None\n",
    "    return match_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 5: `descent_info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent_info(page):\n",
    "    categories = page.categories\n",
    "    descent_list = []\n",
    "\n",
    "    descent_patterns = [\n",
    "        r\"of ([A-Za-z-]+(?: [A-Za-z-]+)?) descent\",\n",
    "        r\"([A-Za-z-]+) emigrants\",\n",
    "        r\"Emigrants from the ([A-Za-z-]+) Empire\",  # russian, german, aus\n",
    "        r\"([A-Za-z-]+) American military personnel\"\n",
    "    ]\n",
    "    descent_groups = [1, 1, 1, 1]\n",
    "\n",
    "    for cat in categories:\n",
    "        matched_str = regex_match(descent_patterns, cat, descent_groups)\n",
    "        # print(matched_str)\n",
    "        if matched_str != None:\n",
    "            descent_list.append(matched_str)\n",
    "\n",
    "    return {\"descent\": descent_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'descent': ['German-Jewish']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'descent': ['French', 'Norwegian']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oppenheimer_page = wikipedia.page('Robert Oppenheimer')\n",
    "chevalier_page = wikipedia.page('Haakon_Chevalier')\n",
    "print(descent_info(oppenheimer_page))\n",
    "descent_info(chevalier_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 6: `education_info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def education_info(page):\n",
    "    categories = page.categories\n",
    "\n",
    "    edu_list = []\n",
    "    edu_patterns = [\n",
    "        r\"([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+) alumni\", # uni Göttingen\n",
    "        r\"Alumni of ([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+)\",\n",
    "    ]\n",
    "    edu_groups = [1, 1]\n",
    "\n",
    "    for cat in categories:\n",
    "        matched_str = regex_match(edu_patterns, cat, edu_groups)\n",
    "        # print(matched_str)\n",
    "        if matched_str != None:\n",
    "            edu_list.append(matched_str)\n",
    "\n",
    "    return {\"education\": edu_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'education': ['Saint Petersburg State University']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'education': [\"Christ's College, Cambridge\",\n",
       "  'Ethical Culture Fieldston School',\n",
       "  'Harvard College',\n",
       "  'University of Göttingen']}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_page = wikipedia.page('Ayn_Rand', auto_suggest=False)\n",
    "print(education_info(rand_page))\n",
    "education_info(oppenheimer_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 7: `conformity_info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "conformity_keywords = [\n",
    "        'atheist', 'anti-communist', 'communist', 'socialist', 'humanist',\n",
    "        'anti-fascist', 'fascist', 'zionist', 'roman catholic', \n",
    "        'marxist','critics of marxism', 'critics of christianity', \n",
    "        'critics of religions'\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformity_info(page):\n",
    "    categories = page.categories\n",
    "    # need \n",
    "    conform_list = []\n",
    "    for cat in categories:\n",
    "        for keyword in conformity_keywords:\n",
    "            if keyword.lower() in cat.lower():\n",
    "                conform_list.append(keyword)\n",
    "                break       # so that does not anti- does not account for communist\n",
    "    \n",
    "    conform_list = list(set(conform_list))\n",
    "\n",
    "    return {'conformity': conform_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conformity': ['zionist', 'anti-communist', 'atheist', 'critics of christianity', 'critics of religions', 'anti-fascist']}\n",
      "{'conformity': ['communist', 'marxist', 'socialist']}\n",
      "{'conformity': ['roman catholic', 'anti-communist']}\n",
      "{'conformity': ['zionist', 'anti-communist', 'atheist', 'critics of christianity', 'critics of religions', 'anti-fascist']}\n"
     ]
    }
   ],
   "source": [
    "kennedy_page = wikipedia.page('John_F._Kennedy', auto_suggest=False)\n",
    "hathaway_page = wikipedia.page('Clarence_Hathaway', auto_suggest=False)\n",
    "print(conformity_info(rand_page))\n",
    "print(conformity_info(hathaway_page))\n",
    "print(conformity_info(kennedy_page))\n",
    "print(conformity_info(rand_page))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 8: `party_info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Ayn_Rand'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rand_page.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def party_info(page):\n",
    "    url = page.url\n",
    "    response = requests.get(url)\n",
    "    bs = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    infobox = bs.find('table', {'class': 'infobox'})\n",
    "    party_list = []\n",
    "\n",
    "    if infobox:\n",
    "        party_row = infobox.find('th', string='Political party')\n",
    "        if party_row:\n",
    "            party_data = party_row.find_next('td')\n",
    "            if party_data:\n",
    "                party_links = party_data.find_all('a')\n",
    "                for link in party_links:\n",
    "                    party_name = link.get_text().strip()\n",
    "                    party_list.append(party_name)\n",
    "\n",
    "        # 'Other political affiliations' not working\n",
    "        other_affiliations_row = infobox.find('th', string=re.compile(r'Other political'))\n",
    "        if other_affiliations_row:\n",
    "            affiliations_data = other_affiliations_row.find_next('td')\n",
    "            if affiliations_data:\n",
    "\n",
    "                affiliations_links = affiliations_data.find_all('a')\n",
    "                for link in affiliations_links:\n",
    "                    affiliation_name = link.get_text().strip()\n",
    "                    party_list.append(affiliation_name)\n",
    "\n",
    "    party_list = list(set(party_list))\n",
    "    if not party_list:\n",
    "        party_list.append(\"DON'T KNOW\")\n",
    "\n",
    "    return {\"party\": party_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'party': ['Republican', 'Democratic']}\n",
      "{'party': ['Republican']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'party': ['Democratic-Farmer-Labor']}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eisenhower_page = wikipedia.page('Dwight Eisenhower', auto_suggest=False)\n",
    "benson_page = wikipedia.page('Elmer_A._Benson', auto_suggest=False)\n",
    "mccarthy_page = wikipedia.page('Joseph Mccarthy', auto_suggest=False)\n",
    "print(party_info(mccarthy_page))\n",
    "print(party_info(eisenhower_page))\n",
    "party_info(benson_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 9: `birthplace_info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geonamescache\n",
    "\n",
    "gc = geonamescache.GeonamesCache()\n",
    "gc_countries = gc.get_countries()\n",
    "gc_us_states = gc.get_us_states()\n",
    "gc_cities = gc.get_cities()\n",
    "\n",
    "country_names = {data['name'] for data in gc_countries.values()}\n",
    "state_names = {data['name'] for data in gc_us_states.values()}\n",
    "city_names = {data['name'] for data in gc_cities.values()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_placenames(place_names):\n",
    "    lower_list = []\n",
    "    input_list = list(place_names)\n",
    "    for i in input_list:\n",
    "        i0 = i.lower()\n",
    "        lower_list.append(i0)\n",
    "    return lower_list\n",
    "country_names0 = lower_placenames(country_names)\n",
    "state_names0 = lower_placenames(state_names)\n",
    "city_names0 = lower_placenames(city_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def birthplace_info(page):\n",
    "    url = page.url\n",
    "    response = requests.get(url)\n",
    "    bs = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # infobox-label\n",
    "    infobox = bs.find('table', {'class': 'infobox'})\n",
    "    if not infobox:\n",
    "        return {\"birthplace\": None, \"birthcity\": None, \"birthstate\": None, \"birthcountry\": None}\n",
    "\n",
    "    birthplace = None\n",
    "    birthcity = None\n",
    "    birthstate = None\n",
    "    birthcountry = None\n",
    "\n",
    "    born_row = infobox.find('th', string=\"Born\")\n",
    "    if born_row:\n",
    "        data_cell = born_row.find_next_sibling('td')\n",
    "        if data_cell:\n",
    "            birthplace_div = data_cell.find('div', {'class': 'birthplace'})\n",
    "            birth_info_text = data_cell.get_text(separator=\" \", strip=True)\n",
    "            # deal with both links and text content in birthplace division\n",
    "            if birthplace_div:\n",
    "                location_parts = []\n",
    "                # print('birth-div')\n",
    "                for item in birthplace_div.contents:\n",
    "                    if item.name == 'a':\n",
    "                        for parts in item.text.split(','):\n",
    "                            location_parts.append(parts.strip().lower())\n",
    "                    elif isinstance(item, str):\n",
    "                        cleaned_parts = []\n",
    "                        for part in item.split(','):\n",
    "                            cleaned_parts.append(part.strip().lower())\n",
    "                        location_parts.extend(cleaned_parts)\n",
    "                # if location_parts[0]:\n",
    "                #     for state in state_names:\n",
    "                #         if state in location_parts[0]:\n",
    "                #             birthstate = state\n",
    "                #             birthcountry = 'USA'\n",
    "                br = 0\n",
    "                br2 = 0\n",
    "                br3 = 0\n",
    "                if isinstance(location_parts, list):\n",
    "                    for place in location_parts:\n",
    "                        # birthplace = location_parts[0]\n",
    "                        if place not in state_names0 and place not in country_names0 and place not in city_names0 and place != 'u.s.' and place != '':\n",
    "                            if br == 0:\n",
    "                                birthplace = place\n",
    "                                # print(place)\n",
    "                                br = 1\n",
    "                        if place in city_names0:\n",
    "                            if br3 == 0:\n",
    "                                birthcity = place\n",
    "                                br3 += 1\n",
    "                                # print(place)\n",
    "                        if place in state_names0:\n",
    "                            birthstate = place\n",
    "                            birthcountry = 'united states'\n",
    "                            br2 = 1\n",
    "                        if br2 == 0 and place in country_names0:\n",
    "                            birthcountry = place\n",
    "            # if class birthplace not found, use spacy GPE\n",
    "            else:\n",
    "                doc = nlp(birth_info_text)\n",
    "                gpe_set = set()\n",
    "                for ent in doc.ents:\n",
    "                    if ent.label_ == 'GPE':\n",
    "                        gpe_str = ent.text.lower()\n",
    "                        gpe_set.add(gpe_str)\n",
    "                # print(gpe_set)\n",
    "                gpe_list = list(gpe_set)\n",
    "                b = 0\n",
    "                if isinstance(gpe_list, list):\n",
    "                    for place in gpe_list:\n",
    "                        if place not in state_names0 and place not in country_names0 and place not in city_names0 and place != 'u.s.':\n",
    "                            birthplace = place\n",
    "                        if place in city_names0:\n",
    "                            birthcity = place\n",
    "                        if place in state_names0:\n",
    "                            birthstate = place\n",
    "                            birthcountry = 'united states'\n",
    "                            b = 1\n",
    "                        if b == 0 and place in country_names0:\n",
    "                            birthcountry = place\n",
    "    return {\n",
    "        \"birthplace\": birthplace,\n",
    "        \"birthcity\": birthcity,\n",
    "        \"birthstate\": birthstate,\n",
    "        \"country\": birthcountry\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'birthplace': 'lakewood township', 'birthcity': None, 'birthstate': 'new jersey', 'country': 'united states'}\n",
      "{'birthplace': 'russian empire', 'birthcity': 'saint petersburg', 'birthstate': None, 'country': None}\n",
      "{'birthplace': None, 'birthcity': 'new orleans', 'birthstate': 'louisiana', 'country': 'united states'}\n",
      "{'birthplace': None, 'birthcity': 'portland', 'birthstate': 'oregon', 'country': 'united states'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'birthplace': 'lamar',\n",
       " 'birthcity': None,\n",
       " 'birthstate': 'missouri',\n",
       " 'country': 'united states'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hellman_page = wikipedia.page('Lillian Hellman', auto_suggest=False)\n",
    "truman_page = wikipedia.page('Harry Truman', auto_suggest=False)\n",
    "print(birthplace_info(chevalier_page))\n",
    "print(birthplace_info(rand_page))\n",
    "print(birthplace_info(hellman_page))\n",
    "print(birthplace_info(may_page))\n",
    "birthplace_info(truman_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 10: `spatial_info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_info(page):\n",
    "    content = page.content\n",
    "    doc = nlp(content)\n",
    "    countries = set()\n",
    "    states = set()\n",
    "    cities = set()\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\":\n",
    "            if ent.text in state_names:\n",
    "                states.add(ent.text)\n",
    "            elif ent.text in country_names:\n",
    "                countries.add(ent.text)\n",
    "            elif ent.text in city_names:\n",
    "                cities.add(ent.text)\n",
    "    \n",
    "    dict0 = {\n",
    "        \"list_countries\": list(countries),\n",
    "        \"list_states\": list(states),\n",
    "        \"list_cities\": list(cities)\n",
    "        }\n",
    "\n",
    "    return dict0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list_countries': ['United States', 'France', 'Greece', 'Netherlands', 'Germany', 'Japan', 'Mexico'], 'list_states': ['Washington', 'New Mexico', 'Oregon', 'New Hampshire', 'New Jersey', 'California', 'Nevada', 'Colorado', 'New York', 'Massachusetts'], 'list_cities': ['Hiroshima', 'Reno', 'Berlin', 'Manhattan', 'Berkeley', 'Dachau', 'Coventry', 'Boston', 'New York City', 'Tokyo', 'Leiden', 'Princeton', 'Seattle', 'Warsaw', 'Pasadena', 'Saint John', 'Los Angeles', 'London', 'Hamburg', 'Alamogordo', 'Exeter', 'Munich', 'Cambridge', 'Nagasaki', 'Dresden']}\n",
      "{'list_countries': ['Canada', 'Australia'], 'list_states': ['Texas', 'Ohio', 'California', 'New York', 'Missouri'], 'list_cities': ['Monterey', 'Culver City', 'Lima', 'Odessa', 'Santa Rosa', 'Boston', 'Ontario', 'Hayward', 'Alameda', 'Los Angeles', 'Jefferson City', 'Oakland', 'Hollywood', 'Toronto', 'Mango']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'list_countries': ['France',\n",
       "  'Austria',\n",
       "  'Switzerland',\n",
       "  'Germany',\n",
       "  'Hungary',\n",
       "  'Poland',\n",
       "  'Canada'],\n",
       " 'list_states': ['Washington',\n",
       "  'Arkansas',\n",
       "  'North Carolina',\n",
       "  'New Hampshire',\n",
       "  'West Virginia',\n",
       "  'California',\n",
       "  'Vermont',\n",
       "  'New York',\n",
       "  'Maryland'],\n",
       " 'list_cities': ['New Haven',\n",
       "  'Prague',\n",
       "  'Boston',\n",
       "  'Moore',\n",
       "  'New York City',\n",
       "  'Baltimore',\n",
       "  'Ottawa',\n",
       "  'Moscow',\n",
       "  'San Francisco',\n",
       "  'Bern',\n",
       "  'Mexico City',\n",
       "  'Vienna',\n",
       "  'Geneva',\n",
       "  'Yalta',\n",
       "  'London',\n",
       "  'Wheeling',\n",
       "  'Harvey']}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiss_page = wikipedia.page('Alger Hiss', auto_suggest=False)\n",
    "print(spatial_info(oppenheimer_page))\n",
    "print(spatial_info(hoskins_page))\n",
    "spatial_info(hiss_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 11: `institutions_info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def institutions_info(page):\n",
    "    categories = page.categories\n",
    "    inst_list = []\n",
    "\n",
    "    inst_patterns = [\n",
    "        r\"([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+) faculty\",\n",
    "        r\"Academics of ([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+)\",\n",
    "        r\"Members of ([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+)\",\n",
    "        r\"([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+) members\",\n",
    "        r\"([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+) politicians\",\n",
    "        r\"Fellows of ([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+) \",\n",
    "        r\"([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+) fellows\",\n",
    "        r\"([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+) officials\",\n",
    "        r\"([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+) informants\",\n",
    "        r\"People of the([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+)\",\n",
    "        r\"([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+) people\",\n",
    "        r\"([A-Za-zÀ-ÖØ-öø-ÿ-\\s&,']+) personnel\"\n",
    "    ]\n",
    "\n",
    "    group_index = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "    for cat in categories:\n",
    "        matched_str = regex_match(inst_patterns, cat, group_index)\n",
    "        # print(matched_str)\n",
    "        if matched_str != None and matched_str != 'American':\n",
    "            inst_list.append(regex_match(inst_patterns, cat, group_index))\n",
    "    \n",
    "    return {\"institutions\": inst_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'institutions': ['Chicago Federation of Labor', 'Industrial Workers of the World', 'Labor Party of the United States', 'the Communist Party USA']}\n",
      "{'institutions': ['United States Department of Agriculture', 'United States Department of State']}\n",
      "{'institutions': ['the American Academy of Arts and', 'the American Academy of Arts and Letters']}\n",
      "{'institutions': ['American Federation of Teachers', 'Chicago Federation of Labor']}\n"
     ]
    }
   ],
   "source": [
    "hammarsmark_page = wikipedia.page('Samuel_Hammersmark', auto_suggest=False)\n",
    "herstein_page = wikipedia.page('Lillian_Herstein', auto_suggest=False)\n",
    "print(institutions_info(hammarsmark_page))\n",
    "print(institutions_info(hiss_page))\n",
    "print(institutions_info(hellman_page))\n",
    "print(institutions_info(herstein_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'institutions': ['the University of Birmingham',\n",
       "  'British',\n",
       "  'Communist Party of Germany',\n",
       "  'Manhattan Project',\n",
       "  'the German Academy of Sciences at Berlin',\n",
       "  'Nuclear weapons program of the Soviet Union',\n",
       "  'Reichsbanner Schwarz-Rot-Gold',\n",
       "  'Socialist Unity Party of Germany']}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuchs_page = wikipedia.page('Klaus_Fuchs', auto_suggest=False)\n",
    "institutions_info(fuchs_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function 12: `norp_info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norp_info(page):\n",
    "    page_links = page.links\n",
    "    links_text = ','.join(page_links)\n",
    "    doc = nlp(links_text)\n",
    "    norp_set = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"NORP\":\n",
    "            norp_set.add(ent.text)\n",
    "    \n",
    "    dict0 = {\n",
    "        \"norp\": list(norp_set)\n",
    "        }\n",
    "\n",
    "    return dict0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'norp': ['Jews', 'Spanish', 'Stalinism', 'Communist', 'Maltese', 'Attic']}\n"
     ]
    }
   ],
   "source": [
    "print(norp_info(hellman_page))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_info(name):\n",
    "    d0 = {'name': name}\n",
    "    page0 = wikipedia.page(name, auto_suggest=False)\n",
    "    categories = page0.categories\n",
    "    d1 = dates_info(page0)\n",
    "    d2 = sexuality_info(page0)\n",
    "    d3 = race_info(page0)\n",
    "    d4 = occupation_info(page0)\n",
    "    d5 = descent_info(page0)\n",
    "    d6 = education_info(page0)\n",
    "    d7 = conformity_info(page0)\n",
    "    d8 = party_info(page0)\n",
    "    d9 = birthplace_info(page0)\n",
    "    d10 = spatial_info(page0)\n",
    "    d11 = institutions_info(page0)\n",
    "    d12 = norp_info(page0)\n",
    "    out_dict = d0 | d1 | d2 | d3 | d4 | d5 | d6 | d7 | d8 | d9 | d10 | d11 | d12\n",
    "    print(f'{name} wiki dictionary created')\n",
    "    \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'Ayn Rand',\n",
       " 'birth_year': 1905,\n",
       " 'death_year': 1982,\n",
       " 'bio_sex': 'FEMALE',\n",
       " 'sexuality': 'NOTA',\n",
       " 'race': 'non-black',\n",
       " 'occupation': 'author',\n",
       " 'descent': ['Russian-Jewish', 'Soviet'],\n",
       " 'education': ['Saint Petersburg State University'],\n",
       " 'conformity': ['zionist',\n",
       "  'anti-communist',\n",
       "  'atheist',\n",
       "  'critics of christianity',\n",
       "  'critics of religions',\n",
       "  'anti-fascist'],\n",
       " 'party': [\"DON'T KNOW\"],\n",
       " 'birthplace': 'russian empire',\n",
       " 'birthcity': 'saint petersburg',\n",
       " 'birthstate': None,\n",
       " 'country': None,\n",
       " 'list_countries': ['Russia', 'United States', 'Israel', 'Norway', 'Canada'],\n",
       " 'list_states': ['California', 'New York'],\n",
       " 'list_cities': ['Saint Petersburg',\n",
       "  'Chicago',\n",
       "  'Los Angeles',\n",
       "  'New York City',\n",
       "  'Hollywood'],\n",
       " 'institutions': [],\n",
       " 'norp': ['Authoritarian',\n",
       "  'Stateless',\n",
       "  'novella),Anti-Communist',\n",
       "  'Restorative',\n",
       "  'Sovereignty',\n",
       "  'European',\n",
       "  'Collectivist',\n",
       "  'American',\n",
       "  'Marxian',\n",
       "  'Modernism',\n",
       "  'Russian Symbolists',\n",
       "  'Soviet',\n",
       "  'Libertarian',\n",
       "  'Capitalist',\n",
       "  'Democratic',\n",
       "  'Imperialism',\n",
       "  'Anglo',\n",
       "  'Islam',\n",
       "  'Democrat',\n",
       "  'Monarchia',\n",
       "  'Abolitionism',\n",
       "  'Feminist',\n",
       "  'Japanese',\n",
       "  'Communalism',\n",
       "  'Objectivist',\n",
       "  'Finnish',\n",
       "  'Mathematics',\n",
       "  'Transcendentalism',\n",
       "  'African',\n",
       "  'Islamism',\n",
       "  'Mutualism',\n",
       "  'Christian',\n",
       "  'Totalitarian',\n",
       "  'Individualist',\n",
       "  'Liberalism',\n",
       "  'Randian',\n",
       "  'Nazism',\n",
       "  'Russian',\n",
       "  'Collectivism',\n",
       "  'Indian',\n",
       "  'Anarchist',\n",
       "  'Agorism',\n",
       "  'Artistic',\n",
       "  'Axiom',\n",
       "  'Green',\n",
       "  'Chanakya',\n",
       "  'Russian Soviet',\n",
       "  'Capitalism',\n",
       "  'Conservatives',\n",
       "  'Consumerism',\n",
       "  'Corporatism',\n",
       "  'Agrarianism',\n",
       "  'Americans',\n",
       "  'Hedonism',\n",
       "  'Jewish',\n",
       "  'Marxist',\n",
       "  'Neoliberal']}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_dict = wiki_info('Ayn Rand')\n",
    "print(type(rand_dict))\n",
    "rand_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years_list = []\n",
    "# for i in huac_df['title']:\n",
    "#     years_dict = get_life_years(i)\n",
    "#     years_list.append(years_dict)\n",
    "\n",
    "# years_df = pd.DataFrame(years_list)\n",
    "# years_df.to_csv('data/494years.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list494 = []\n",
    "\n",
    "# for i in huac_df['title']:\n",
    "#     wiki_dict = wiki_info(i)\n",
    "#     list494.append(wiki_dict)\n",
    "\n",
    "# df494 = pd.DataFrame(list494)\n",
    "# df494.to_csv('data-tolera/494wiki.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
